{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54af9b99-7d54-43db-9d85-55f0ce861095",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to define the models\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91478aad-f67b-4510-8734-5d3d8e748b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Device: cuda:0\n",
      "NVIDIA GeForce 940MX\n",
      "Total GPU Memory 4240703488 B, Reserved GPU Memory 0 B, Allocated GPU Memory 0 B\n"
     ]
    }
   ],
   "source": [
    "#To get GPU details\n",
    "\n",
    "import torch\n",
    "\n",
    "def get_gpu_details():\n",
    "    t = torch.cuda.get_device_properties(0).total_memory\n",
    "    r = torch.cuda.memory_reserved(0)\n",
    "    a = torch.cuda.memory_allocated(0)\n",
    "    print(torch.cuda.get_device_name())\n",
    "    print(f'Total GPU Memory {t} B, Reserved GPU Memory {r} B, Allocated GPU Memory {a} B')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f'Current Device: {device}')\n",
    "\n",
    "if device == 'cuda:0':\n",
    "    torch.cuda.empty_cache()\n",
    "    get_gpu_details()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c053433e-445f-4591-9812-74d72943a16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Load vocab\n",
    "with open('Model/vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "# Load model\n",
    "model_path = \"Model/trained_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7966de09-74c6-42b1-976a-b7ddc5c34c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(embed_size, embed_size)\n",
    "        self.keys = nn.Linear(embed_size, embed_size)\n",
    "        self.queries = nn.Linear(embed_size, embed_size)\n",
    "        self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query, mask):\n",
    "        # Get number of training examples\n",
    "        N = query.shape[0]\n",
    "\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        values = self.values(values)  # (N, value_len, embed_size)\n",
    "        keys = self.keys(keys)  # (N, key_len, embed_size)\n",
    "        queries = self.queries(query)  # (N, query_len, embed_size)\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = queries.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        # Einsum does matrix mult. for query*keys for each training example\n",
    "        # with every other training example, don't be confused by einsum\n",
    "        # it's just how I like doing matrix multiplication & bmm\n",
    "\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        # queries shape: (N, query_len, heads, heads_dim),\n",
    "        # keys shape: (N, key_len, heads, heads_dim)\n",
    "        # energy: (N, heads, query_len, key_len)\n",
    "\n",
    "        # Mask padded indices so their weights become 0\n",
    "        if mask is not None:\n",
    "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # Normalize energy values similarly to seq2seq + attention\n",
    "        # so that they sum to 1. Also divide by scaling factor for\n",
    "        # better stability\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "        # attention shape: (N, heads, query_len, key_len)\n",
    "        # values shape: (N, value_len, heads, heads_dim)\n",
    "        # out after matrix multiply: (N, query_len, heads, head_dim), then\n",
    "        # we reshape and flatten the last two dimensions.\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        # Linear layer doesn't modify the shape, final shape will be\n",
    "        # (N, query_len, embed_size)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41815eca-aa4c-433a-bc49-f794935ee9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        device,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        max_length,\n",
    "    ):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    heads,\n",
    "                    dropout=dropout,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        out = self.dropout(\n",
    "            (self.word_embedding(x) + self.position_embedding(positions))\n",
    "        )\n",
    "\n",
    "        # In the Encoder the query, key, value are all the same, it's in the\n",
    "        # decoder this will change. This might look a bit odd in this case.\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d482bbc3-4455-4547-af8b-7458972d35c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        trg_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        heads,\n",
    "        forward_expansion,\n",
    "        dropout,\n",
    "        device,\n",
    "        max_length,\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.device = device\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(embed_size, heads, forward_expansion, dropout, device)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        N, seq_length = x.shape\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "        x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        out = self.fc_out(x)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88a889ab-e091-4199-a8a5-98038ad15de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.attention = SelfAttention(embed_size, heads=heads)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, heads, dropout, forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        attention = self.attention(x, x, x, trg_mask)\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dda82ebd-a129-4e6c-a266-03887b4f583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention = SelfAttention(embed_size, heads)\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, value, key, query, mask):\n",
    "        attention = self.attention(value, key, query, mask)\n",
    "\n",
    "        # Add skip connection, run through normalization and finally dropout\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "105483a2-73b3-4123-ab14-990a7ebf9c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        forward_expansion,\n",
    "        heads,\n",
    "        dropout,\n",
    "        device,\n",
    "        max_length=100,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(-2).unsqueeze(-2)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(1, 1, trg_len, trg_len)\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg=None):\n",
    "        if trg is not None:\n",
    "            # Training\n",
    "            src_mask = self.make_src_mask(src)\n",
    "            trg_mask = self.make_trg_mask(trg)\n",
    "            enc_src = self.encoder(src, src_mask)\n",
    "            output = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "            return output\n",
    "        else:\n",
    "            # Inference\n",
    "            return self.inference(src)\n",
    "\n",
    "    # def inference(self, src):\n",
    "    #     src_mask = self.make_src_mask(src)\n",
    "    #     enc_src = self.encoder(src, src_mask)\n",
    "    #     trg_init_token = torch.tensor([[self.trg_pad_idx]]).to(src.device)  # <pad> token to start decoding\n",
    "    #     trg_tokens = [trg_init_token]\n",
    "    #     for i in range(self.max_length):\n",
    "    #         trg_tensor = torch.cat(trg_tokens, dim=1)\n",
    "    #         trg_mask = self.make_trg_mask(trg_tensor)\n",
    "    #         output = self.decoder(trg_tensor, enc_src, src_mask, trg_mask)\n",
    "    #         pred_token = output.argmax(dim=-1)[:,-1].unsqueeze(1)  # Predict next token\n",
    "    #         trg_tokens.append(pred_token)\n",
    "    #         if pred_token.item() == self.trg_pad_idx:\n",
    "    #             break\n",
    "    #     return torch.cat(trg_tokens, dim=1)\n",
    "    def inference(self, src):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        trg_init_token = torch.tensor([[self.trg_pad_idx]]).to(src.device)  # <pad> token to start decoding\n",
    "        trg_tokens = [trg_init_token]\n",
    "        for i in range(self.max_length):\n",
    "            trg_tensor = torch.cat(trg_tokens, dim=1)\n",
    "            trg_mask = self.make_trg_mask(trg_tensor)\n",
    "            output = self.decoder(trg_tensor, enc_src, src_mask, trg_mask)\n",
    "            pred_token = output.argmax(dim=-1)[:, -1].unsqueeze(1)  # Predict next token\n",
    "            trg_tokens.append(pred_token)\n",
    "            if pred_token.item() == self.trg_pad_idx:\n",
    "                break\n",
    "        return torch.cat(trg_tokens, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11652794-73ab-4842-97da-075e4e499dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        forward_expansion,\n",
    "        heads,\n",
    "        dropout,\n",
    "        device,\n",
    "        max_length,\n",
    "    ):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.max_length = max_length  # Set max_length as an attribute\n",
    "        \n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            device,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            device,\n",
    "            max_length,\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "    \n",
    "    def make_src_mask(self, src):\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(-2).unsqueeze(-2)\n",
    "        return src_mask.to(self.device)\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(1, 1, trg_len, trg_len)\n",
    "        return trg_mask.to(self.device)\n",
    "\n",
    "    def forward(self, src, trg=None):\n",
    "        if trg is not None:\n",
    "            # Training\n",
    "            src_mask = self.make_src_mask(src)\n",
    "            trg_mask = self.make_trg_mask(trg)\n",
    "            enc_src = self.encoder(src, src_mask)\n",
    "            output = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "            return output\n",
    "        else:\n",
    "            # Inference\n",
    "            return self.inference(src)\n",
    "\n",
    "    # def inference(self, src):\n",
    "    #     src_mask = self.make_src_mask(src)\n",
    "    #     enc_src = self.encoder(src, src_mask)\n",
    "    #     trg_init_token = torch.tensor([[self.trg_pad_idx]]).to(src.device)  # <pad> token to start decoding\n",
    "    #     trg_tokens = [trg_init_token]\n",
    "    #     for i in range(self.max_length):\n",
    "    #         trg_tensor = torch.cat(trg_tokens, dim=1)\n",
    "    #         trg_mask = self.make_trg_mask(trg_tensor)\n",
    "    #         output = self.decoder(trg_tensor, enc_src, src_mask, trg_mask)\n",
    "    #         pred_token = output.argmax(dim=-1)[:,-1].unsqueeze(1)  # Predict next token\n",
    "    #         trg_tokens.append(pred_token)\n",
    "    #         if pred_token.item() == self.trg_pad_idx:\n",
    "    #             break\n",
    "    #     return torch.cat(trg_tokens, dim=1)\n",
    "    def inference(self, src):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        trg_init_token = torch.tensor([[self.trg_pad_idx]]).to(src.device)  # <pad> token to start decoding\n",
    "        trg_tokens = [trg_init_token]\n",
    "        for i in range(self.max_length):\n",
    "            trg_tensor = torch.cat(trg_tokens, dim=1)\n",
    "            trg_mask = self.make_trg_mask(trg_tensor)\n",
    "            output = self.decoder(trg_tensor, enc_src, src_mask, trg_mask)\n",
    "            pred_token = output.argmax(dim=-1)[:, -1].unsqueeze(1)  # Predict next token\n",
    "            trg_tokens.append(pred_token)\n",
    "            if pred_token.item() == self.trg_pad_idx:\n",
    "                break\n",
    "        return torch.cat(trg_tokens, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bacdcf9-1340-4b80-9021-551cba845db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "embed_size = 256\n",
    "num_layers = 4\n",
    "forward_expansion = 4\n",
    "heads =4\n",
    "dropout = 0.019\n",
    "learning_rate = 0.01\n",
    "num_epochs = 20\n",
    "max_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57a1a4da-1be3-4872-9873-b9e3923fe209",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    src_vocab_size=len(vocab),\n",
    "    trg_vocab_size=len(vocab),\n",
    "    src_pad_idx=vocab[\"<pad>\"],\n",
    "    trg_pad_idx=vocab[\"<pad>\"],\n",
    "    embed_size=embed_size,\n",
    "    num_layers=num_layers,\n",
    "    forward_expansion=forward_expansion,\n",
    "    heads=heads,\n",
    "    dropout=dropout,\n",
    "    device=device,\n",
    "    max_length=max_length \n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb17c782-9721-4a24-9c5b-f465466463d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming vocab has a method or attribute to access index-to-token mapping\n",
    "index_to_token = {index: vocab.itos[index] for index in range(len(vocab))}\n",
    "\n",
    "# Function to convert tensor of numerical indices to string\n",
    "def tensor_to_string(tensor, index_to_token, pad_token='<pad>'):\n",
    "    tokens = [index_to_token[index.item()] for index in tensor if index.item() != vocab[pad_token]]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def convert_string(value):\n",
    "    question_tensor = torch.tensor(value)\n",
    "    # question_tensor = torch.tensor(value).clone().detach()\n",
    "    question_string = tensor_to_string(question_tensor, index_to_token)\n",
    "    return question_string\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a9c3c92-e27d-495f-8541-d46cebc3ef39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'maidens schedule pine by the entrance along plated with the agama appeal of the nyatapola temple , way a drawing they purpose the there remarkable and the festival .'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act_val = [3398, 3249, 1610,   29,    2,  245,  402, 4031,   19,    2,  575, 1887,\n",
    "            3,    2,   18,   10,    5, 1207,   13, 2240,  272,  148,    2,   62,\n",
    "          761,    7,    2,  145,    4,    1,    1,    1,    1,    1,    1,    1,\n",
    "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
    "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
    "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
    "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
    "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
    "            1,    1,    1]\n",
    "\n",
    "convert_string(act_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cce1a5d-c3cd-4a3b-9830-f22fb6fd1684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chowkot salvaged entrance by the entrance material divided with the nyatapola appeal of nyatapola nyatapola temple , who a drawing geography with the community remarkable and eyes community of sage of of of of . the the of the its its its . durvasa the in with with the the the . the the the with the the and the its . the its . . , durvasa with the . the condition the . the the of the the the . the the the the the the the the the and the . with the the . the'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_val =[1299, 1376,  245,   29,    2,  245,  705,  824,   19,    2,   18, 1887,\n",
    "            3,   18,   18,   10,    5,   28,   13, 2240, 1906,   19,    2,  113,\n",
    "          761,    7, 1171,  113,    3,  507,    3,    3,    3,    3,    4,    2,\n",
    "            2,    3,    2,   27,   27,   27,    4,  147,    2,    8,   19,   19,\n",
    "            2,    2,    2,    4,    2,    2,    2,   19,    2,    2,    7,    2,\n",
    "           27,    4,    2,   27,    4,    4,    5,  147,   19,    2,    4,    2,\n",
    "          152,    2,    4,    2,    2,    3,    2,    2,    2,    4,    2,    2,\n",
    "            2,    2,    2,    2,    2,    2,    2,    7,    2,    4,   19,    2,\n",
    "            2,    4,    2]\n",
    "\n",
    "convert_string(pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a30e58ba-67fe-4657-9d05-68f22d6440b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reconstruction temple is a to windows reconstruction by , a is site as a 000 such role the and roof . it to . . in . the . in . . . . . the . in . . . its . . . . . . the . . . . . the . . . . . . . to . . . . . . . . bhaktapur . . . the . . them . the . the . . . . . , , them . the . . post . . .'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1 = [  56,   10,    9,   13,   11,   38,   56,   29,    5,   13,    9,  253,\n",
    "           16,   13, 1011,  104,   70,    2,    7,  315,    4,   33,   11,    4,\n",
    "            4,    8,    4,    2,    4,    8,    4,    4,    4,    4,    4,    2,\n",
    "            4,    8,    4,    4,    4,   27,    4,    4,    4,    4,    4,    4,\n",
    "            2,    4,    4,    4,    4,    4,    2,    4,    4,    4,    4,    4,\n",
    "            4,    4,   11,    4,    4,    4,    4,    4,    4,    4,    4,   15,\n",
    "            4,    4,    4,    2,    4,    4,  897,    4,    2,    4,    2,    4,\n",
    "            4,    4,    4,    4,    5,    5,  897,    4,    2,    4,    4,  527,\n",
    "            4,    4,    4]\n",
    "\n",
    "convert_string(pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9926345-a279-4d3f-b86a-4339179a9dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the pieces depicting of how as a final in srinath , describe the building melodious and involvement hoist in gate wall .'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "act1 =   [   2, 2551,  693,    3,   23,   16,   13,  908,    8,  260,    5,  366,\n",
    "            2,  124, 3202,    7,  962, 1996,    8,   50,  127,    4,    1,    1,\n",
    "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
    "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
    "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
    "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
    "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
    "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
    "            1,    1,    1]\n",
    "\n",
    "convert_string(act1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ba624a9-3da1-45ea-99b0-2e1c27841357",
   "metadata": {},
   "outputs": [],
   "source": [
    "testq = \"What is the height of Nyatapola ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "081f9ce2-44f1-4bdb-ac74-d9d494bcb236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the height of nyatapola ?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/abhishek/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/abhishek/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/abhishek/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary resources for NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_sentence(sentence):\n",
    "    # Lowercase and strip\n",
    "    sentence = sentence.lower().strip()\n",
    "    # Creating a space between a word and the punctuation following it\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    # Removing contractions\n",
    "    sentence = re.sub(r\"i'm\", \"i am\", sentence)\n",
    "    sentence = re.sub(r\"he's\", \"he is\", sentence)\n",
    "    sentence = re.sub(r\"she's\", \"she is\", sentence)\n",
    "    sentence = re.sub(r\"it's\", \"it is\", sentence)\n",
    "    sentence = re.sub(r\"that's\", \"that is\", sentence)\n",
    "    sentence = re.sub(r\"what's\", \"that is\", sentence)\n",
    "    sentence = re.sub(r\"where's\", \"where is\", sentence)\n",
    "    sentence = re.sub(r\"how's\", \"how is\", sentence)\n",
    "    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
    "    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
    "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "    sentence = re.sub(r\"won't\", \"will not\", sentence)\n",
    "    sentence = re.sub(r\"can't\", \"cannot\", sentence)\n",
    "    sentence = re.sub(r\"n't\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"n'\", \"ng\", sentence)\n",
    "    sentence = re.sub(r\"'bout\", \"about\", sentence)\n",
    "    # Replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,0-9 %]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "\n",
    "# Stop words removal and lemmatization function\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stop words\n",
    "    #stop_words = set(stopwords.words(\"english\"))\n",
    "    #filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens if word.lower()]\n",
    "    return \" \".join(lemmatized_tokens)\n",
    "\n",
    "# Apply preprocessing to each question and answer\n",
    "testq = preprocess_sentence(testq)\n",
    "testq = preprocess_text(testq)\n",
    "\n",
    "print(testq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4cbd8948-8694-474a-afaf-a562d267d27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   1, 4547, 6720, 2523, 2846,  923, 6665, 5329, 2485, 3898, 3577,  337,\n",
      "         6897,  509, 3706, 6706,  401, 1976, 3746, 2295,   71, 2508, 6267, 4227,\n",
      "         3621, 2866, 3408, 2090, 6113,  289, 2810, 2949, 3269, 1447, 1279, 6879,\n",
      "         6987, 6945, 1355, 2129, 1169, 2888, 5488, 1176, 1255, 2902, 6875, 3294,\n",
      "         4562, 5706, 1224, 1656, 6959, 5772, 5123, 2477, 2646, 4159, 6745, 3408,\n",
      "          367, 5586, 2505, 2419, 1495, 3096, 2754, 5908, 1542, 2761, 6545, 4634,\n",
      "         3444, 6038, 1811, 2239, 1878, 6363, 4019, 6938, 3408, 6189, 5536, 1471,\n",
      "         2295, 5814,  304, 2194,  858, 4113, 6137, 5983,  886, 6669, 6666, 2343,\n",
      "          814,  863, 3471, 6274, 5496]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "class TestQADataset(Dataset):\n",
    "    def __init__(self, question, tokenizer, vocab, max_seq_length):\n",
    "        self.question = question\n",
    "        self.vocab = vocab\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        question_tokens = self.tokenizer(self.question)\n",
    "        question_indices = [self.vocab[token] for token in question_tokens]\n",
    "\n",
    "        # Pad or truncate sequence to max_seq_length\n",
    "        question_indices = question_indices[:self.max_seq_length] + [self.vocab['<pad>']] * (self.max_seq_length - len(question_indices))\n",
    "        question_tensor = torch.tensor(question_indices, dtype=torch.long)\n",
    "\n",
    "        return question_tensor\n",
    "\n",
    "\n",
    "# Define max sequence length\n",
    "max_seq_length = 100\n",
    "\n",
    "# Convert string data into tensors\n",
    "test_dataset = TestQADataset(testq, tokenizer, vocab, max_seq_length)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 1  # Set batch size to 1 for testing\n",
    "\n",
    "# Create data loader\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Example usage of the test loader\n",
    "for idx, question in enumerate(test_loader):\n",
    "    # Move the tensor to the appropriate device\n",
    "    question = question.to(device)  # Assuming `device` is defined elsewhere\n",
    "\n",
    "    # Perform inference with the model using the question\n",
    "    with torch.no_grad():\n",
    "        output_sequence = model(question)\n",
    "\n",
    "print(output_sequence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89aec8aa-808d-4652-a116-ab0a2d89051c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting tensor to list \n",
    "output_sequence = output_sequence.tolist()\n",
    "print(output_sequence)\n",
    "\n",
    "# Flatten the nested list\n",
    "output_sequence= [item for sublist in output_sequence for item in sublist]\n",
    "print(output_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e3f1cfa-0616-48a6-8857-644cbd9e20e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming vocab has a method or attribute to access index-to-token mapping\n",
    "index_to_token = {index: vocab.itos[index] for index in range(len(vocab))}\n",
    "\n",
    "# Function to convert tensor of numerical indices to string\n",
    "def tensor_to_string(tensor, index_to_token, pad_token='<pad>'):\n",
    "    tokens = [index_to_token[index.item()] for index in tensor if index.item() != vocab[pad_token]]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def convert_string(value):\n",
    "    question_tensor = torch.tensor(value)\n",
    "    # question_tensor = torch.tensor(value).clone().detach()\n",
    "    question_string = tensor_to_string(question_tensor, index_to_token)\n",
    "    return question_string\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f7fdf300-9da0-40b7-9b12-f6d431f0770c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([36], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Getting highest probability\n",
    "high_output_sequence = output_sequence.argmax(dim=-1)\n",
    "print(high_output_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eb1dc689-eb20-4e50-b2f9-70ada48927cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from\n"
     ]
    }
   ],
   "source": [
    "predicted_indices = high_output_sequence\n",
    "\n",
    "# Convert predicted token indices to tokens\n",
    "predicted_tokens = [vocab.itos[index] for index in high_output_sequence]\n",
    "\n",
    "# Handle end-of-sequence token\n",
    "eos_index = vocab.stoi['<eos>']\n",
    "if eos_index in predicted_indices:\n",
    "    eos_position = (predicted_indices == eos_index).nonzero(as_tuple=True)[0]\n",
    "    predicted_tokens = predicted_tokens[:eos_position + 1]\n",
    "\n",
    "# Post-processing (if needed)\n",
    "# For example, remove padding tokens\n",
    "predicted_tokens = [token for token in predicted_tokens if token != '<pad>']\n",
    "\n",
    "# Join tokens into a single string (if needed)\n",
    "predicted_answer = ' '.join(predicted_tokens)\n",
    "\n",
    "# Print or use predicted answer\n",
    "print(predicted_answer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c6ae14b-c90b-4cc1-87ed-67cefd20a9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'creatures renunciant dates manifests drunk hotter solution mangsir tribes gem multi badrinatha school storytelling featured metalwork threads evolve alternative privately responsible interwoven located marked drunk agni 60 well possible mul thicker pratyavijafia promise reference gilded took plan connects realizing mark starting emancipated crumbs upanisad 50 shower doa imbued styled eras agamdev honour well brings devastation ganesh keen liveliness visitors jumgam retrofit masan generation inspection spot nside commencement somewhat digambara shameful umamaheswara plan sudden jayasthiti organize acquiring adhaar patan spot multi lun . precursor rebuilt dushyanta paramount insulting nightlife vainya atris harmoniously cited able decisions endeavors resulting plan featured fascination restorations'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_string(output_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a0989a-68cf-4e88-9700-d6cf26faa1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
